<!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.0 Transitional//EN" "https://www.w3.org/TR/xhtml1/DTD/xhtml1-transitional.dtd">
<html xmlns="http://www.w3.org/1999/xhtml">
<head>
<meta http-equiv="Content-Type" content="text/xhtml;charset=UTF-8"/>
<meta http-equiv="X-UA-Compatible" content="IE=9"/>
<meta name="generator" content="Doxygen 1.8.17"/>
<meta name="viewport" content="width=device-width, initial-scale=1"/>
<title>TX9 Mobile Robot: Perception Module: ACME TX9 Mobile Robot: Perception Module</title>
<link href="tabs.css" rel="stylesheet" type="text/css"/>
<script type="text/javascript" src="jquery.js"></script>
<script type="text/javascript" src="dynsections.js"></script>
<link href="search/search.css" rel="stylesheet" type="text/css"/>
<script type="text/javascript" src="search/searchdata.js"></script>
<script type="text/javascript" src="search/search.js"></script>
<link href="doxygen.css" rel="stylesheet" type="text/css" />
</head>
<body>
<div id="top"><!-- do not remove this div, it is closed by doxygen! -->
<div id="titlearea">
<table cellspacing="0" cellpadding="0">
 <tbody>
 <tr style="height: 56px;">
  <td id="projectalign" style="padding-left: 0.5em;">
   <div id="projectname">TX9 Mobile Robot: Perception Module
   &#160;<span id="projectnumber">2.0</span>
   </div>
   <div id="projectbrief">Acme Robotics is building a next generation mobile robot to be used on sports fields to detect and track players and provide analytics data about them to be utilized for broadcasting. A swarm of robots will be deployed on the field.</div>
  </td>
 </tr>
 </tbody>
</table>
</div>
<!-- end header part -->
<!-- Generated by Doxygen 1.8.17 -->
<script type="text/javascript">
/* @license magnet:?xt=urn:btih:cf05388f2679ee054f2beb29a391d25f4e673ac3&amp;dn=gpl-2.0.txt GPL-v2 */
var searchBox = new SearchBox("searchBox", "search",false,'Search');
/* @license-end */
</script>
<script type="text/javascript" src="menudata.js"></script>
<script type="text/javascript" src="menu.js"></script>
<script type="text/javascript">
/* @license magnet:?xt=urn:btih:cf05388f2679ee054f2beb29a391d25f4e673ac3&amp;dn=gpl-2.0.txt GPL-v2 */
$(function() {
  initMenu('',true,false,'search.php','Search');
  $(document).ready(function() { init_search(); });
});
/* @license-end */</script>
<div id="main-nav"></div>
<!-- window showing the filter options -->
<div id="MSearchSelectWindow"
     onmouseover="return searchBox.OnSearchSelectShow()"
     onmouseout="return searchBox.OnSearchSelectHide()"
     onkeydown="return searchBox.OnSearchSelectKey(event)">
</div>

<!-- iframe showing the search results (closed by default) -->
<div id="MSearchResultsWindow">
<iframe src="javascript:void(0)" frameborder="0" 
        name="MSearchResults" id="MSearchResults">
</iframe>
</div>

</div><!-- top -->
<div class="PageDoc"><div class="header">
  <div class="headertitle">
<div class="title">ACME TX9 Mobile Robot: Perception Module </div>  </div>
</div><!--header-->
<div class="contents">
<div class="textblock"><p><a href="https://app.travis-ci.com/SumedhKoppula/TX9PerceptionModule"><object type="image/svg+xml" data="https://app.travis-ci.com/SumedhKoppula/TX9PerceptionModule.svg?branch=devBranchPhaseII" style="pointer-events: none;">Build Status</object></a></p>
<p><a href="https://coveralls.io/github/SumedhKoppula/TX9PerceptionModule?branch=main"><object type="image/svg+xml" data="https://coveralls.io/repos/github/SumedhKoppula/TX9PerceptionModule/badge.svg?branch=devBranchPhaseII" style="pointer-events: none;">Coverage Status</object></a></p>
<h1><a class="anchor" id="autotoc_md2"></a>
@ref LICENSE.md "![GitHub license](https://badgen.net/github/license/Naereen/Strapdown.js)"</h1>
<hr  />
<h1><a class="anchor" id="autotoc_md4"></a>
Overview</h1>
<p>Acme Robotics is building a next generation mobile robot to be used on sports fields to detect and track players and provide analytics data about them to be utilized for broadcasting. A swarm of robots will be deployed on the field.</p>
<p>The mobile robot will have a monocular camera placed on it to receive live video feed of the field. A perception module on the robot will receive the video feed and will label each player and store their location with respect to the robot. The output of the module will be the labels and location of the players. This will be fed to a path planner, so that the mobile robot can track a player to ensure that the player is in its field of vision.</p>
<h2><a class="anchor" id="autotoc_md5"></a>
Video Links</h2>
<p>Phase 1: <a href="https://www.youtube.com/watch?v=6h-72SBKB8E&amp;t=72s">Video link</a> Phase 2: <a href="https://www.youtube.com/watch?v=6h-72SBKB8E&amp;t=72s">Video link</a></p>
<h2><a class="anchor" id="autotoc_md6"></a>
AIP Document</h2>
<p><a href="https://docs.google.com/spreadsheets/d/1xySDFZ3LB8iaYRV7OXNlhccmeYK0k-ZA7r3EBrgCAMA/edit?usp=sharing"><img src="https://readthedocs.org/projects/ansicolortags/badge/?version=latest" alt="Documentation Status" class="inline"/></a></p>
<h1><a class="anchor" id="autotoc_md7"></a>
Sprint Planning Review Document</h1>
<p><a href="https://docs.google.com/document/d/1LRK0pHuJsmCjaX9VIXsoMOvOonWKOmAHq0_yvjCOSTk/edit?usp=sharing"><img src="https://readthedocs.org/projects/ansicolortags/badge/?version=latest" alt="Documentation Status" class="inline"/></a></p>
<h1><a class="anchor" id="autotoc_md8"></a>
Assumptions</h1>
<ol type="1">
<li>For deep learning computation we assume that <em>Nvidia Jetson Nano 2GB</em> chipset</li>
<li>The targets are assumed to be 180cm in height, which will be used to calculate distance from the robot in the forward direction.</li>
</ol>
<h1><a class="anchor" id="autotoc_md9"></a>
Technologies</h1>
<p><em>Programing language</em>: C++</p>
<p><em>Build system</em>: cmake</p>
<p><em>Testing Library</em>: Google Test, Google Mock</p>
<p><em>Continuous Integration</em>: Travis CI, Coverall</p>
<h1><a class="anchor" id="autotoc_md10"></a>
Algorithms</h1>
<ol type="1">
<li>YOLO v3 under YOLO LICENSE</li>
<li>OpenCV version 4.2, under Apache 2 License</li>
</ol>
<h2><a class="anchor" id="autotoc_md11"></a>
YOLO Functionality</h2>
<p>The YOLO algorithm uses convolutional neural networks (CNN) to detect multi objects in real-time. This algorithm uses a single forward propagation through a neural network to detect objects in a single run. Here the role of CNN is to simultaneously predict various class probabilities and bounding boxes. Object detection in YOLO is done as a regression problem and provides the class probabilities of the detected images.</p>
<p><img src="https://user-images.githubusercontent.com/24978535/136276058-9714fecf-60d9-4164-b8c6-25416cbfbb2b.png" alt="Midterm_Proposal_img1" class="inline"/></p>
<p>Figure: input video being processed by YOLO vision algorithms outputting boxed video as image detection.</p>
<h2><a class="anchor" id="autotoc_md12"></a>
OpenCV</h2>
<p>An open-source library for computer vision, machine learning, and image processing. Some of the header files which we would like to access in our project are below:</p>
<p>#include &lt;opencv2/dnn.hpp&gt;</p>
<p>#include &lt;opencv2/imgproc.hpp&gt;</p>
<p>#include &lt;opencv2/highgui.hpp&gt;</p>
<p>#include &lt;opencv2/opencv.hpp&gt;</p>
<h1><a class="anchor" id="autotoc_md13"></a>
Risk and Mitigations</h1>
<p>Risk 1: The created software fails to detect humans. Solution: OpenCv will be used to detect humans based on the jersey color and the dimensions of the bounding box. Utilizing the dimensions of the bounding box will ensure that false positives are minimalized. The average height assumed will be modified to include only the size of an average human torso.</p>
<p>Risk 2: The software fails to track the detected humans Solution: The final product will be delivered only for human detection and the tracking update will be provided in a future release.</p>
<h1><a class="anchor" id="autotoc_md14"></a>
Testing:</h1>
<p>Googleâ€™s Open Images Dataset V6+ will be used for testing and quality assurance. The dataset contains labelled images of humans with the information about bounding boxes provided in it. For real time testing, a laptop and webcam will be used and the output will be monitored to ensure proper functioning.</p>
<h1><a class="anchor" id="autotoc_md15"></a>
Standard install via command-line</h1>
<div class="fragment"><div class="line">git clone --recursive https://github.com/pratik-a99/TX9PerceptionModule</div>
<div class="line">cd &lt;path to repository&gt;</div>
<div class="line">bash script.sh</div>
<div class="line">mkdir build</div>
<div class="line">cd build</div>
<div class="line">cmake ..</div>
<div class="line">make</div>
<div class="line">Run tests: ./test/cpp-test</div>
<div class="line">Run program: ./app/shell-app --image=../image.jpg</div>
</div><!-- fragment --><h1><a class="anchor" id="autotoc_md16"></a>
Building for code coverage</h1>
<div class="fragment"><div class="line">sudo apt-get install lcov</div>
<div class="line">cmake -D COVERAGE=ON -D CMAKE_BUILD_TYPE=Debug ../</div>
<div class="line">make</div>
<div class="line">make code_coverage</div>
</div><!-- fragment --><p>This generates a index.html page in the build/coverage sub-directory that can be viewed locally in a web browser. </p>
</div></div><!-- contents -->
</div><!-- PageDoc -->
<!-- start footer part -->
<hr class="footer"/><address class="footer"><small>
Generated by &#160;<a href="http://www.doxygen.org/index.html">
<img class="footer" src="doxygen.png" alt="doxygen"/>
</a> 1.8.17
</small></address>
</body>
</html>
